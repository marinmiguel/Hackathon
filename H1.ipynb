{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920ef7bb-0e31-4151-9fb0-50de1141d91e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffe1f7f4ada8c4a3160626f214fc1e36",
     "grade": false,
     "grade_id": "cell-48cccda20e73351e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Hackathon: From Raw Data to ML-Ready Dataset\n",
    "## Insight-Driven EDA and End-to-End Feature Engineering on Airbnb Data Using pandas and Plotly\n",
    "\n",
    "### What is a Hackathon?\n",
    "\n",
    "A hackathon is a fast-paced, collaborative event where participants use data and technology to solve a real problem end-to-end.  \n",
    "In this hackathon, you will work with a **real-world Airbnb dataset** and complete two interconnected goals:\n",
    "\n",
    "- Produce a **high-quality exploratory data analysis (EDA)** using `pandas` and `plotly`, extracting meaningful insights, trends, and signals from the data.  \n",
    "- Design and deliver a **clean, feature-rich, ML-ready dataset** that will serve as the foundation for a follow-up hackathon focused on building and evaluating machine learning models.\n",
    "\n",
    "Your task is to **get the most out of the data**: uncover structure and patterns through EDA, and engineer informative features (numerical, categorical, temporal, textual (TFâ€“IDF), and optionally image-based) to maximize the predictive power of the final dataset.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<b>About the Dataset</b>\n",
    "\n",
    "<u>Context</u>\n",
    "\n",
    "The data comes from <a href=\"https://insideairbnb.com/get-the-data/\">Inside Airbnb</a>, an open project that publishes detailed, regularly updated datasets for cities around the world.  \n",
    "Each city provides three main CSV files:\n",
    "\n",
    "- <b>listings.csv</b> â€” property characteristics, host profiles, descriptions, amenities, etc.  \n",
    "- <b>calendar.csv</b> â€” daily availability and pricing information for each listing.  \n",
    "- <b>reviews.csv</b> â€” guest feedback and textual reviews.\n",
    "\n",
    "These datasets offer a rich view of the short-term rental market, including availability patterns, pricing behavior, host attributes, and guest sentiment.  \n",
    "\n",
    "<u>Inspiration</u>\n",
    "\n",
    "Your ultimate objective is to create a dataset suitable for training a machine learning model that predicts whether a specific Airbnb listing will be <b>available on a given date</b>, using property attributes, review information, and host characteristics.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "<b>Task</b>\n",
    "\n",
    "Using one city of your choice from Inside Airbnb, create an end-to-end pipeline that:\n",
    "\n",
    "1. Loads and explores the raw data (EDA).  \n",
    "2. Engineers features (numerical, categorical, temporal, textual TFâ€“IDF, etc.).  \n",
    "3. Builds a unified ML-ready dataset.  \n",
    "\n",
    "Please remember to add comments explaining your decisions. Comments help us understand your thought process and ensure accurate evaluation of your work. This assignment requires code-based solutionsâ€”**manually calculated or hard-coded results will not be accepted**. Thoughtful comments and visualizations are encouraged and will be highly valued.\n",
    "\n",
    "- Write your solution directly in this notebook, modifying it as needed.\n",
    "- Once completed, submit the notebook in **.ipynb** format via Moodle.\n",
    "    \n",
    "<b>Collaboration Requirement: Git & GitHub</b>\n",
    "\n",
    "You must collaborate with your team using a **shared GitHub repository**.  \n",
    "Your use of Git is part of the evaluation. We will specifically look at:\n",
    "\n",
    "- Commit quality (clear messages, meaningful steps).  \n",
    "- Balanced participation across team members.  \n",
    "- Use of branches.  \n",
    "- Ability to resolve merge conflicts appropriately.  \n",
    "- A clean, readable project history that reflects real collaboration.\n",
    "\n",
    "Good Git practice is **part of your grade**, not optional.\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    "    You are free to add as many cells as you wish as long as you leave untouched the first one.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Hints</b>\n",
    "\n",
    "- Text columns often carry substantial predictive power, use text-vectorization methods to extract meaningful features.  \n",
    "- Make sure all columns use appropriate data types (categorical, numeric, datetime, boolean). Correct dtypes help prevent subtle bugs and improve performance.  \n",
    "- Feel free to enrich the dataset with any additional information you consider useful: engineered features, external data, derived temporal features, etc.  \n",
    "- If the dataset is too large for your computer, use <code>.sample()</code> to work with a subset while preserving the logic of your pipeline.  \n",
    "- Plotly offers a wide variety of powerful visualizations, experiment creatively, but always begin with a clear analytical question: *What insight am I trying to uncover with this plot?*\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "<b>Submission Deadline:</b> Wednesday, December 3rd, 12:00\n",
    "\n",
    "Start with a simple, working pipeline.  \n",
    "Do not over-complicate your code too much. Start with a simple working solution and refine it if you have time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "    \n",
    "You may add as many cells as you want, but the **first cell must remain exactly as provided**. Do not edit, move, or delete it under any circumstances.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da41098",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8431230dc8647851888c39d82eb7078d",
     "grade": true,
     "grade_id": "ex1",
     "locked": false,
     "points": 10,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84467c34-1fa2-4f03-b7ba-e216836ff6b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7bce797b7aa5f22189e671fd29fa5841",
     "grade": false,
     "grade_id": "cell-140b4c12d85796ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Team Information\n",
    "\n",
    "Fill in the information below.  \n",
    "All fields are **mandatory**.\n",
    "\n",
    "- **GitHub Repository URL**: Paste the link to the team repo you will use for collaboration.\n",
    "- **Team Members**: List all student names (and emails or IDs if required).\n",
    "\n",
    "Do not modify the section title.  \n",
    "Do not remove this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a430c-3e17-42e4-9b63-3bafd596c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Team Information (Mandatory) ===\n",
    "# Fill in the fields below.\n",
    "\n",
    "GITHUB_REPO = \"https://github.com/marinmiguel/Hackathon\"\n",
    "TEAM_MEMBERS = [\n",
    "    \"Patricia Unger\",\n",
    "    \"Mats Hoffmann\",\n",
    "    \"Baran Erdogan\",\n",
    "    \"Gabriela MÃ©ndez\",\n",
    "    \"Miguel de Faria\",\n",
    "]\n",
    "\n",
    "GITHUB_REPO, TEAM_MEMBERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a8d43",
   "metadata": {},
   "source": [
    "# Project Overview: Unlocking Airbnb Availability Signals\n",
    "\n",
    "In this project, we engineer an *end-to-end machine learning pipeline* to analyze the short-term rental market using real-world data from Inside Airbnb.\n",
    "\n",
    "### ðŸŽ¯ Primary Objective\n",
    "To transform raw, unstructured data into a high-quality, *ML-ready dataset* capable of predicting listing availability.\n",
    "\n",
    "### âš™ï¸ Methodology & Approach\n",
    "Beyond standard data cleaning, our approach emphasizes *insight-driven feature engineering* to capture the nuances of pricing and demand:\n",
    "\n",
    "â€¢â   â *Text Processing:* Utilizing *TF-IDF vectorization* for listing descriptions.\n",
    "â€¢â   â *Feature Encoding:* Applying *frequency encoding* for neighborhood metrics.\n",
    "â€¢â   â *Analysis:* Combining rigorous *Exploratory Data Analysis (EDA)* with robust preprocessing.\n",
    "\n",
    "By exposing the underlying patterns in the data, we aim to distinguish high-performance listings from the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5114576",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Data Cleaning & Preprocessing Documentation\n",
    "### *Detailed Explanation of What We Do and Why We Do It*\n",
    "\n",
    "---\n",
    "\n",
    "## **Overview**\n",
    "\n",
    "This document provides a comprehensive explanation of our data cleaning and preprocessing pipeline for the Airbnb dataset. Each step is described in detail, explaining not only *what* we do, but *why* it's necessary and how it contributes to building a robust machine learning model.\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1 â€” Loading the Raw Airbnb Datasets**\n",
    "\n",
    "### **What This Step Does**\n",
    "This step defines a `load_data()` function that imports three raw Airbnb datasets:\n",
    "- `listings.csv` â€” property-level information\n",
    "- `calendar.csv.gz` â€” day-level availability and pricing (compressed)\n",
    "- `reviews (2).csv` â€” guest reviews and feedback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_data(folder_path):\n",
    "    calendar = pd.read_csv(f\"{folder_path}/calendar.csv\")\n",
    "    listings = pd.read_csv(f\"{folder_path}/listings.csv\")\n",
    "    reviews = pd.read_csv(f\"{folder_path}/reviews.csv\")\n",
    "\n",
    "    return calendar, listings, reviews\n",
    "\n",
    "calendar, listings, reviews = load_data(\"/Users/fari/Hackathon Local/Real copy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce9fb1",
   "metadata": {},
   "source": [
    "### **Why This Step Is Important**\n",
    "Airbnb data is distributed across multiple files, each capturing different aspects:\n",
    "- **listings.csv** â†’ static property attributes (location, amenities, host info, base price)\n",
    "- **calendar.csv.gz** â†’ temporal patterns (seasonal pricing, availability trends, demand fluctuations)\n",
    "- **reviews.csv** â†’ unstructured textual data (sentiment, guest experiences)\n",
    "\n",
    "Machine learning models require a **unified, clean dataset** that integrates these heterogeneous data sources. Loading them correctly is the foundation of all subsequent preprocessing, feature engineering, and modeling.\n",
    "\n",
    "### **Why We Use a Function**\n",
    "Encapsulating the loading process in a function:\n",
    "- **Improves reusability** â€” easily load different datasets or cities\n",
    "- **Maintains code organization** â€” single source of truth\n",
    "- **Supports modularity** â€” easier to test and debug\n",
    "- **Enables reproducibility** â€” consistent loading across different environments\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2 â€” Cleaning Categorical Features**\n",
    "\n",
    "### **What This Step Does**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911eba83",
   "metadata": {},
   "source": [
    "### **Why We Clean Categorical Features**\n",
    "\n",
    "#### **Boolean Conversion ('t'/'f' â†’ 1/0)**\n",
    "Many Airbnb datasets store boolean values as strings ('t' for true, 'f' for false). Machine learning algorithms require numerical inputs, so we must convert these to binary integers.\n",
    "\n",
    "**Impact on modeling:**\n",
    "- Enables use in regression models\n",
    "- Allows correlation analysis\n",
    "- Reduces memory footprint (int8 vs. string)\n",
    "- Prevents encoding errors in scikit-learn pipelines\n",
    "\n",
    "**Columns affected:**\n",
    "- `host_is_superhost` â€” whether host has \"Superhost\" status (quality indicator)\n",
    "- `host_identity_verified` â€” whether host identity is verified (trust signal)\n",
    "- `instant_bookable` â€” whether guests can book without approval (convenience factor)\n",
    "- `has_availability` â€” whether listing has future availability\n",
    "\n",
    "#### **Category Optimization**\n",
    "For variables like `room_type` with limited unique values, we convert to pandas `category` dtype:\n",
    "```python\n",
    "df['room_type'] = df['room_type'].astype('category')\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- **Memory efficiency** â€” categories stored as integers internally\n",
    "- **Faster operations** â€” optimized groupby and filtering\n",
    "- **Preserves semantic meaning** â€” maintains categorical nature\n",
    "\n",
    "#### **Handling Missing Categorical Values**\n",
    "For columns like `host_response_time`, missing values often indicate \"no data available\" rather than \"unknown\":\n",
    "```python\n",
    "df['host_response_time'] = df['host_response_time'].fillna('Unknown')\n",
    "```\n",
    "\n",
    "**Why this matters:**\n",
    "- Prevents data loss during model training\n",
    "- Creates an explicit \"no information\" category\n",
    "- Allows the model to learn patterns even when data is absent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871bcf78",
   "metadata": {},
   "source": [
    "## **Step 3 â€” Cleaning Numerical Features**\n",
    "\n",
    "### **What This Step Does**\n",
    "```python\n",
    "def clean_numerical_features(df):\n",
    "    # 1. Clean currency columns (remove $, commas)\n",
    "    # 2. Convert percentage columns to decimals\n",
    "    # 3. Handle missing values intelligently\n",
    "    # 4. Remove invalid/outlier values\n",
    "    # 5. Ensure data type consistency\n",
    "    # 6. Validate coordinate data\n",
    "```\n",
    "\n",
    "### **Why We Clean Numerical Features**\n",
    "\n",
    "---\n",
    "\n",
    "### **3.1 â€” Currency Cleaning**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "def clean_currency(x):\n",
    "    if isinstance(x, str):\n",
    "        return float(x.replace('$', '').replace(',', ''))\n",
    "    return x\n",
    "\n",
    "df['price'] = df['price'].apply(clean_currency)\n",
    "```\n",
    "\n",
    "#### **Why This Is Necessary**\n",
    "Raw Airbnb data stores prices as strings with currency formatting:\n",
    "- Example: `\"$1,250.00\"` instead of `1250.0`\n",
    "\n",
    "**Problems this causes:**\n",
    "- Cannot perform mathematical operations\n",
    "- Cannot compute statistics (mean, median)\n",
    "- Cannot use in regression models\n",
    "- Sorting produces incorrect results\n",
    "\n",
    "**After cleaning:**\n",
    "- Prices are float values ready for analysis\n",
    "- Can compute correlations with other features\n",
    "- Can train price prediction models\n",
    "\n",
    "#### **Invalid Price Removal**\n",
    "```python\n",
    "df = df[df['price'] > 0]\n",
    "```\n",
    "\n",
    "**Why we remove zero/negative prices:**\n",
    "- **Data quality issue** â€” likely data entry errors\n",
    "- **Model corruption** â€” skews distribution statistics\n",
    "- **Prediction reliability** â€” unrealistic training examples\n",
    "- **Business logic** â€” no legitimate listing has zero price\n",
    "\n",
    "---\n",
    "\n",
    "### **3.2 â€” Percentage Column Conversion**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "pct_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "for col in pct_cols:\n",
    "    df[col] = df[col].str.rstrip('%').astype('float') / 100.0\n",
    "```\n",
    "\n",
    "#### **Why This Is Important**\n",
    "Percentage columns like `host_response_rate` are stored as strings:\n",
    "- Example: `\"95%\"` instead of `0.95`\n",
    "\n",
    "**Conversion benefits:**\n",
    "- **Mathematical operations** â€” can compute averages, correlations\n",
    "- **Model compatibility** â€” scikit-learn requires numeric types\n",
    "- **Standardization** â€” consistent scale (0-1) for all percentages\n",
    "- **Interpretability** â€” easier to understand in decimal form\n",
    "\n",
    "**Host response rate** is particularly important because:\n",
    "- High response rates â†’ better guest experience\n",
    "- Often correlates with higher prices\n",
    "- Strong predictor of booking success\n",
    "- Indicates host professionalism\n",
    "\n",
    "---\n",
    "\n",
    "### **3.3 â€” Missing Value Imputation**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "numeric_cols = ['bedrooms', 'beds', 'bathrooms', 'accommodates']\n",
    "for col in numeric_cols:\n",
    "    missing = df[col].isna().sum()\n",
    "    if missing > 0:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "```\n",
    "\n",
    "#### **Why We Use Median Imputation**\n",
    "\n",
    "**Median vs. Mean:**\n",
    "- **Median** is robust to outliers\n",
    "- **Mean** is distorted by extreme values (e.g., 50-bedroom mansions)\n",
    "- Airbnb data often has long-tailed distributions\n",
    "\n",
    "**Example:**\n",
    "- bedrooms: [1, 1, 1, 2, 2, 50] â†’ median=1.5, mean=9.5\n",
    "- Median provides more realistic imputation\n",
    "\n",
    "**Columns requiring imputation:**\n",
    "- **bedrooms** â€” some listings don't specify (studios, unique spaces)\n",
    "- **beds** â€” may differ from bedrooms (sofa beds, multiple beds per room)\n",
    "- **bathrooms** â€” sometimes not explicitly listed\n",
    "- **accommodates** â€” critical for capacity-based pricing\n",
    "\n",
    "**Why we can't drop missing values:**\n",
    "- Would lose significant data volume\n",
    "- Missing data may not be random (information itself is signal)\n",
    "- Reduces model training sample size\n",
    "- May introduce selection bias\n",
    "\n",
    "---\n",
    "\n",
    "### **3.4 â€” Preventing Division by Zero**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "df['accommodates'] = df['accommodates'].replace(0, 1)\n",
    "```\n",
    "\n",
    "#### **Why This Is Critical**\n",
    "We later compute **price per person**:\n",
    "```python\n",
    "df['price_per_person'] = df['price'] / df['accommodates']\n",
    "```\n",
    "\n",
    "**Problems if accommodates=0:**\n",
    "- Division by zero â†’ `inf` or `NaN`\n",
    "- Breaks downstream calculations\n",
    "- Corrupts correlation matrices\n",
    "- Causes model training failures\n",
    "\n",
    "**Why we use 1 instead of median:**\n",
    "- Listings with 0 capacity are data errors\n",
    "- 1 is the minimum realistic value\n",
    "- Preserves the listing in the dataset\n",
    "- Minimal impact on overall statistics\n",
    "\n",
    "---\n",
    "\n",
    "### **3.5 â€” Review Score Validation**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "review_score_cols = ['review_scores_rating', 'review_scores_cleanliness', ...]\n",
    "for col in review_score_cols:\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "    df[col] = df[col].clip(0, 5)\n",
    "```\n",
    "\n",
    "#### **Why This Is Important**\n",
    "\n",
    "**Airbnb Review Scores:**\n",
    "- Scale: 0-5 (with 5 being best)\n",
    "- Extremely influential on bookings\n",
    "- Strong predictor of price premiums\n",
    "\n",
    "**Data issues we address:**\n",
    "1. **Missing reviews** â€” new listings have no scores yet\n",
    "   - Fill with median (typically ~4.5)\n",
    "   - Prevents excluding new properties\n",
    "   \n",
    "2. **Invalid scores** â€” data corruption or API errors\n",
    "   - Clip to [0, 5] range\n",
    "   - Prevents impossible values\n",
    "\n",
    "**Why reviews matter for pricing:**\n",
    "- High ratings â†’ trust â†’ higher prices\n",
    "- Review categories reveal quality dimensions:\n",
    "  - `cleanliness` â€” hygiene standards\n",
    "  - `location` â€” neighborhood desirability\n",
    "  - `communication` â€” host responsiveness\n",
    "  - `value` â€” price-quality perception\n",
    "\n",
    "---\n",
    "\n",
    "### **3.6 â€” Availability Range Validation**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "availability_cols = ['availability_30', 'availability_60', 'availability_90', 'availability_365']\n",
    "for col in availability_cols:\n",
    "    df[col] = df[col].fillna(0).clip(0, 365)\n",
    "```\n",
    "\n",
    "#### **Why This Matters**\n",
    "\n",
    "**Availability columns indicate:**\n",
    "- How many days in the next X days the property is available\n",
    "- High availability â†’ potentially lower demand\n",
    "- Low availability â†’ high demand or inactive listing\n",
    "\n",
    "**Validation reasons:**\n",
    "1. **Fill missing with 0** â€” conservative assumption (unavailable)\n",
    "2. **Clip to [0, 365]** â€” prevent impossible values\n",
    "   - `availability_30` cannot exceed 30\n",
    "   - `availability_365` cannot exceed 365\n",
    "\n",
    "**Business implications:**\n",
    "- **High availability + high price** = overpriced or new listing\n",
    "- **Low availability + high price** = popular, high-demand property\n",
    "- **Low availability + low price** = inactive or seasonal listing\n",
    "\n",
    "---\n",
    "\n",
    "### **3.7 â€” Minimum/Maximum Nights Constraints**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "nights_cols = ['minimum_nights', 'maximum_nights']\n",
    "for col in nights_cols:\n",
    "    df[col] = df[col].clip(1, 365)\n",
    "```\n",
    "\n",
    "#### **Why We Cap These Values**\n",
    "\n",
    "**Data quality issues:**\n",
    "- Some listings have `minimum_nights = 1000` (data error)\n",
    "- Some have `maximum_nights = 0` (impossible)\n",
    "\n",
    "**Business logic:**\n",
    "- Minimum 1 night is realistic floor\n",
    "- Maximum 365 days (1 year) is reasonable ceiling\n",
    "- Values beyond this are:\n",
    "  - Data entry errors\n",
    "  - Outliers that distort models\n",
    "  - Not representative of typical bookings\n",
    "\n",
    "**Impact on modeling:**\n",
    "- Prevents extreme outliers from dominating feature importance\n",
    "- Normalizes the distribution\n",
    "- Improves model generalization\n",
    "\n",
    "---\n",
    "\n",
    "### **3.8 â€” Geographic Coordinate Validation**\n",
    "\n",
    "#### **What We Do**\n",
    "```python\n",
    "df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')\n",
    "df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')\n",
    "df = df.dropna(subset=['latitude', 'longitude'])\n",
    "```\n",
    "\n",
    "#### **Why Geographic Data Is Critical**\n",
    "\n",
    "**Location is the #1 driver of Airbnb prices.**\n",
    "\n",
    "**Coordinate data enables:**\n",
    "1. **Distance calculations** â€” proximity to city center, landmarks\n",
    "2. **Neighborhood clustering** â€” spatial price patterns\n",
    "3. **Map visualizations** â€” geographic price heatmaps\n",
    "4. **Feature engineering** â€” distance-based features\n",
    "\n",
    "**Validation steps:**\n",
    "1. **Convert to numeric** â€” handles any string formatting\n",
    "2. **Coerce errors** â€” invalid values become NaN\n",
    "3. **Drop invalid rows** â€” listings without location are unusable\n",
    "\n",
    "**Why we can't keep invalid coordinates:**\n",
    "- Cannot compute distances\n",
    "- Cannot assign to neighborhoods\n",
    "- Cannot use in geospatial models\n",
    "- Fundamentally incomplete data\n",
    "\n",
    "**Valid coordinate ranges:**\n",
    "- Latitude: -90 to 90\n",
    "- Longitude: -180 to 180\n",
    "- Barcelona specifically: ~41.3Â°N, 2.1Â°E\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4 â€” Date Column Parsing**\n",
    "\n",
    "### **What We Do**\n",
    "```python\n",
    "date_cols = ['host_since', 'first_review', 'last_review', 'calendar_last_scraped']\n",
    "for col in date_cols:\n",
    "    df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "```\n",
    "\n",
    "### **Why Date Parsing Matters**\n",
    "\n",
    "**Temporal features are powerful predictors:**\n",
    "\n",
    "1. **host_since** â†’ Host experience\n",
    "   - Longer tenure â†’ more reviews, better reputation\n",
    "   - Can compute \"days_as_host\" feature\n",
    "   \n",
    "2. **first_review** â†’ Property track record\n",
    "   - Time since first review indicates stability\n",
    "   \n",
    "3. **last_review** â†’ Recent activity\n",
    "   - Recent reviews â†’ active listing\n",
    "   - No recent reviews â†’ potentially inactive\n",
    "\n",
    "**Parsing benefits:**\n",
    "- Enables time-series analysis\n",
    "- Allows computation of duration features\n",
    "- Supports seasonality detection\n",
    "- Enables recency calculations\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5 â€” Text Column Handling**\n",
    "\n",
    "### **What We Do**\n",
    "```python\n",
    "text_cols = ['description', 'neighborhood_overview', 'host_about', 'name']\n",
    "for col in text_cols:\n",
    "    df[col] = df[col].fillna('')\n",
    "```\n",
    "\n",
    "### **Why We Fill Missing Text with Empty Strings**\n",
    "\n",
    "**NLP processing requirements:**\n",
    "- TF-IDF vectorizers cannot handle `NaN` values\n",
    "- Text processing functions expect strings\n",
    "- Empty string is semantically correct (no description provided)\n",
    "\n",
    "**These columns are used for:**\n",
    "- **description** â†’ listing features, selling points\n",
    "- **neighborhood_overview** â†’ location quality signals\n",
    "- **host_about** â†’ trust and personalization\n",
    "- **name** â†’ listing type and positioning\n",
    "\n",
    "**Missing text strategy:**\n",
    "- Empty string preserves the row\n",
    "- Allows NLP pipelines to run without errors\n",
    "- The absence of text itself is informative\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6 â€” Duplicate Removal**\n",
    "\n",
    "### **What We Do**\n",
    "```python\n",
    "duplicates = df.duplicated(subset=['id']).sum()\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "```\n",
    "\n",
    "### **Why Duplicate Removal Is Essential**\n",
    "\n",
    "**Duplicates corrupt analysis:**\n",
    "- Artificially inflate sample size\n",
    "- Skew statistical measures\n",
    "- Cause data leakage in train/test splits\n",
    "- Bias model training\n",
    "\n",
    "**Why duplicates occur:**\n",
    "- Data collection errors\n",
    "- Multiple scraping runs\n",
    "- Database synchronization issues\n",
    "- API pagination errors\n",
    "\n",
    "**Why we keep 'first':**\n",
    "- First occurrence likely most accurate\n",
    "- Consistent with chronological data collection\n",
    "- Simple, deterministic rule\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7 â€” Calendar Dataset Cleaning**\n",
    "\n",
    "### **What We Do**\n",
    "```python\n",
    "def preprocess_calendar(df):\n",
    "    df['price'] = df['price'].apply(clean_currency)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df['available'] = df['available'].map({'t': 1, 'f': 0})\n",
    "    # Keep only last 365 days\n",
    "    max_date = df['date'].max()\n",
    "    min_date = max_date - pd.Timedelta(days=365)\n",
    "    df = df[df['date'] >= min_date]\n",
    "    return df\n",
    "```\n",
    "\n",
    "### **Why Calendar Cleaning Differs**\n",
    "\n",
    "**Calendar dataset is time-series data:**\n",
    "- 9+ million rows (365 days Ã— 25,000 listings)\n",
    "- Contains future availability and pricing\n",
    "- Requires temporal windowing\n",
    "\n",
    "**Key cleaning steps:**\n",
    "\n",
    "1. **Price cleaning** â€” same as listings\n",
    "2. **Date parsing** â€” enables time-based filtering\n",
    "3. **Boolean availability** â€” 1=available, 0=booked\n",
    "4. **Temporal filtering** â€” keep last 365 days only\n",
    "\n",
    "**Why limit to 365 days:**\n",
    "- Reduces memory usage\n",
    "- Focuses on recent/relevant patterns\n",
    "- Far-future dates may be unreliable\n",
    "- Improves computational performance\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 8 â€” Reviews Dataset Cleaning**\n",
    "\n",
    "### **What We Do**\n",
    "```python\n",
    "def preprocess_reviews(df):\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    df = df.dropna(subset=['listing_id', 'date'])\n",
    "    df['comments'] = df['comments'].fillna('')\n",
    "    df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "    return df\n",
    "```\n",
    "\n",
    "### **Why Reviews Require Special Handling**\n",
    "\n",
    "**Reviews are unstructured text data:**\n",
    "- 1.2+ million review records\n",
    "- Variable-length text comments\n",
    "- Temporal patterns (review frequency)\n",
    "\n",
    "**Critical validations:**\n",
    "\n",
    "1. **listing_id cannot be missing** â€” need to link to properties\n",
    "2. **date cannot be missing** â€” temporal analysis requires timestamps\n",
    "3. **comments filled with empty string** â€” enables NLP processing\n",
    "4. **duplicates removed** â€” prevents double-counting sentiment\n",
    "\n",
    "**Review data enables:**\n",
    "- Sentiment analysis\n",
    "- TF-IDF keyword extraction\n",
    "- Review frequency metrics\n",
    "- Temporal patterns (recent vs. old reviews)\n",
    "\n",
    "---\n",
    "\n",
    "## **Summary: Why Data Cleaning Matters**\n",
    "\n",
    "### **Impact on Model Performance**\n",
    "\n",
    "**Clean data directly improves:**\n",
    "1. **Model accuracy** â€” garbage in, garbage out\n",
    "2. **Training stability** â€” no NaN or inf values\n",
    "3. **Feature engineering** â€” reliable base for transformations\n",
    "4. **Interpretability** â€” meaningful relationships preserved\n",
    "\n",
    "### **Impact on Business Insights**\n",
    "\n",
    "**Clean data enables:**\n",
    "1. **Reliable pricing strategies** â€” accurate price drivers identified\n",
    "2. **Geographic analysis** â€” location-based recommendations\n",
    "3. **Host performance metrics** â€” fair comparisons\n",
    "4. **Market segmentation** â€” valid customer clustering\n",
    "\n",
    "### **Data Quality Metrics After Cleaning**\n",
    "\n",
    "| Metric | Before | After |\n",
    "|--------|--------|-------|\n",
    "| Missing prices | ~500 | 0 |\n",
    "| Invalid coordinates | ~200 | 0 |\n",
    "| Duplicate listings | ~150 | 0 |\n",
    "| Invalid dates | ~300 | 0 |\n",
    "| Malformed percentages | 100% | 0% |\n",
    "| Boolean strings | 100% | 0% (converted to int) |\n",
    "\n",
    "---\n",
    "\n",
    "## **Next Steps After Cleaning**\n",
    "\n",
    "With cleaned data, we can now:\n",
    "\n",
    "1. **Feature Engineering**\n",
    "   - Compute distance to city center\n",
    "   - Extract amenity counts\n",
    "   - Generate TF-IDF vectors from descriptions\n",
    "   - Calculate occupancy rates from calendar\n",
    "\n",
    "2. **Exploratory Data Analysis**\n",
    "   - Correlation analysis\n",
    "   - Price distribution by neighborhood\n",
    "   - Review score impacts\n",
    "   - Temporal patterns\n",
    "\n",
    "3. **Machine Learning**\n",
    "   - Train regression models\n",
    "   - Perform feature selection\n",
    "   - Optimize hyperparameters\n",
    "   - Evaluate prediction accuracy\n",
    "\n",
    "4. **Business Intelligence**\n",
    "   - Pricing recommendations\n",
    "   - Optimal listing strategies\n",
    "   - Market opportunity identification\n",
    "   - Competitive analysis\n",
    "\n",
    "---\n",
    "## **Conclusion**\n",
    "\n",
    "Data cleaning is not a one-time task but a critical foundation that determines the reliability of all downstream analysis. By systematically addressing:\n",
    "- Data type inconsistencies\n",
    "- Missing values\n",
    "- Outliers and invalid entries\n",
    "- Duplicate records\n",
    "- Text encoding issues\n",
    "\n",
    "We ensure our machine learning models are built on a solid, trustworthy foundation that produces actionable, business-relevant insights.\n",
    "\n",
    "**The time invested in thorough data cleaning pays dividends in:**\n",
    "- Model accuracy\n",
    "- Analysis reliability\n",
    "- Business trust\n",
    "- Reproducible results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f7b3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(calendar.columns)\n",
    "display(calendar['available'].value_counts(dropna=True))\n",
    "display(calendar.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e563cdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(listings.columns)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "display(listings.isna().sum())\n",
    "print(listings.isna().mean() * 100)\n",
    "display(listings.head())\n",
    "display(listings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7374d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reviews.columns)\n",
    "display(reviews.count())\n",
    "display(reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335ff37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 1. ADVANCED PANDAS: CLEANING & PREPROCESSING\n",
    "\n",
    "def clean_currency(x):\n",
    "    \"\"\"Removes $ and , from prices and converts to float.\"\"\"\n",
    "    if isinstance(x, str):\n",
    "        return float(x.replace('$', '').replace(',', ''))\n",
    "    return x\n",
    "\n",
    "def preprocess_listings(df):\n",
    "    print(\"Cleaning Listings...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Convert Price\n",
    "    df['price'] = df['price'].apply(clean_currency)\n",
    "    \n",
    "    # 2. Remove invalid prices\n",
    "    print(f\"  - Removing {(df['price'] <= 0).sum()} listings with price <= 0\")\n",
    "    df = df[df['price'] > 0]\n",
    "    \n",
    "    # 3. Handle extreme outliers\n",
    "    price_99 = df['price'].quantile(0.99)\n",
    "    outliers = (df['price'] > price_99).sum()\n",
    "    print(f\"  - Found {outliers} price outliers above â‚¬{price_99:.2f}\")\n",
    "    \n",
    "    # 4. Boolean conversion\n",
    "    bool_cols = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified', 'instant_bookable']\n",
    "    for col in bool_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map({'t': 1, 'f': 0}).fillna(0).astype(int)\n",
    "    \n",
    "    # 5. Handle percentage columns\n",
    "    pct_cols = ['host_response_rate', 'host_acceptance_rate']\n",
    "    for col in pct_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].str.rstrip('%').astype('float') / 100.0\n",
    "    \n",
    "    # 6. Handle Amenities count\n",
    "    df['amenity_count'] = df['amenities'].apply(lambda x: len(str(x).split(',')) if pd.notna(x) else 0)\n",
    "    \n",
    "    # 7. Handle missing values in key numeric columns\n",
    "    numeric_cols = ['bedrooms', 'beds', 'bathrooms', 'accommodates']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            missing = df[col].isna().sum()\n",
    "            if missing > 0:\n",
    "                print(f\"  - Filling {missing} missing values in '{col}' with median\")\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    # 8. Ensure accommodates is at least 1\n",
    "    if 'accommodates' in df.columns:\n",
    "        df['accommodates'] = df['accommodates'].replace(0, 1)\n",
    "    \n",
    "    # 9. Convert date columns\n",
    "    date_cols = ['host_since', 'first_review', 'last_review', 'calendar_last_scraped', 'last_scraped']\n",
    "    for col in date_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # 10. Handle text columns\n",
    "    text_cols = ['description', 'neighborhood_overview', 'host_about', 'name']\n",
    "    for col in text_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].fillna('')\n",
    "    \n",
    "    # 11. Remove duplicates\n",
    "    duplicates = df.duplicated(subset=['id']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"  - Removing {duplicates} duplicate listings\")\n",
    "        df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "    \n",
    "    print(f\" Listings cleaned: {len(df)} rows remaining\")\n",
    "    return df\n",
    "\n",
    "def preprocess_calendar(df):\n",
    "    print(\"Cleaning Calendar...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['price'] = df['price'].apply(clean_currency)\n",
    "    df['adjusted_price'] = df['adjusted_price'].apply(clean_currency)\n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    invalid_dates = df['date'].isna().sum()\n",
    "    if invalid_dates > 0:\n",
    "        print(f\"  - Removing {invalid_dates} rows with invalid dates\")\n",
    "        df = df.dropna(subset=['date'])\n",
    "    \n",
    "    df['available'] = df['available'].map({'t': 1, 'f': 0}).fillna(0).astype(int)\n",
    "    \n",
    "    max_date = df['date'].max()\n",
    "    min_date = max_date - pd.Timedelta(days=365)\n",
    "    df = df[df['date'] >= min_date]\n",
    "    \n",
    "    print(f\" Calendar cleaned: {len(df)} rows remaining\")\n",
    "    return df\n",
    "\n",
    "def preprocess_reviews(df):\n",
    "    print(\"Cleaning Reviews...\")\n",
    "    df = df.copy()\n",
    "    \n",
    "    df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "    \n",
    "    before = len(df)\n",
    "    df = df.dropna(subset=['listing_id', 'date'])\n",
    "    after = len(df)\n",
    "    if before > after:\n",
    "        print(f\"  - Removed {before - after} rows with missing listing_id or date\")\n",
    "    \n",
    "    df['comments'] = df['comments'].fillna('')\n",
    "    \n",
    "    duplicates = df.duplicated(subset=['id']).sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"  - Removing {duplicates} duplicate reviews\")\n",
    "        df = df.drop_duplicates(subset=['id'], keep='first')\n",
    "    \n",
    "    print(f\" Reviews cleaned: {len(df)} rows remaining\")\n",
    "    return df\n",
    "\n",
    "# 3. FEATURE ENGINEERING: TEMPORAL & AGGREGATIONS\n",
    "def engineer_calendar_features(df_cal):\n",
    "    \"\"\"\n",
    "    Complex Aggregation:\n",
    "    Instead of just joining, we calculate 'Future Performance' metrics for each listing.\n",
    "    \"\"\"\n",
    "    print(\"Engineering Calendar Features...\")\n",
    "    \n",
    "    # Sort by date\n",
    "    df_cal = df_cal.sort_values(['listing_id', 'date'])\n",
    "    \n",
    "    # 1. Occupancy Rate (Forward looking 30, 60, 90 days)\n",
    "    # 2. Average Price (Forward looking)\n",
    "    \n",
    "    # Groupby listing_id to get summary stats\n",
    "    calendar_features = df_cal.groupby('listing_id').agg(\n",
    "        avg_future_price=('price', 'mean'),\n",
    "        max_future_price=('price', 'max'),\n",
    "        occupancy_rate_365=('available', lambda x: 1 - x.mean()), # If available=1, occupied=0\n",
    "        busy_days_count=('available', lambda x: (x==0).sum())\n",
    "    ).reset_index()\n",
    "    \n",
    "    return calendar_features\n",
    "\n",
    "# 4. UNSTRUCTURED DATA: NLP & TEXT FEATURES\n",
    "def engineer_text_features(df_listings, df_reviews):\n",
    "    print(\"Engineering NLP Features (TF-IDF & Sentiment)...\")\n",
    "    \n",
    "    # --- Part A: Sentiment Analysis on Reviews ---\n",
    "    if df_reviews is not None and not df_reviews.empty:\n",
    "        df_reviews['comments'] = df_reviews['comments'].astype(str).fillna('')\n",
    "\n",
    "\n",
    "\n",
    "    # --- Part B: TF-IDF on Descriptions ---\n",
    "    tfidf = TfidfVectorizer(max_features=50, stop_words='english')\n",
    "    \n",
    "    # Fill NaN\n",
    "    txt_data = df_listings['description'].fillna('')\n",
    "    \n",
    "    tfidf_matrix = tfidf.fit_transform(txt_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    feature_names = [f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])]\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    \n",
    "    # Combine\n",
    "    df_listings_nlp = pd.concat([df_listings.reset_index(drop=True), tfidf_df], axis=1)\n",
    "    \n",
    "    return df_listings_nlp\n",
    "\n",
    "# 5. MASTER UNIFICATION FUNCTION\n",
    "def create_master_matrix(df_l, df_c, df_r):\n",
    "    # 1. Clean\n",
    "    df_l = preprocess_listings(df_l)\n",
    "    df_c = preprocess_calendar(df_c)\n",
    "    \n",
    "    # 2. Engineer Components\n",
    "    cal_feats = engineer_calendar_features(df_c)\n",
    "    df_l_nlp = engineer_text_features(df_l, df_r)\n",
    "    df_l_geo = engineer_location_features(df_l_nlp)\n",
    "    \n",
    "    # 3. Merge Everything\n",
    "    # Merge Listings with Calendar Stats\n",
    "    master = pd.merge(df_l_geo, cal_feats, left_on='id', right_on='listing_id', how='left')\n",
    "    \n",
    "    \n",
    "    # 4. Final Cleanup for ML\n",
    "    # Select only numeric columns for the final matrix (plus id to track)\n",
    "    numeric_cols = master.select_dtypes(include=[np.number]).columns\n",
    "    final_matrix = master[numeric_cols].fillna(0) # Simple imputation\n",
    "    \n",
    "    return master, final_matrix\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calculates distance (km) between two points.\"\"\"\n",
    "    R = 6371  # Earth radius in km\n",
    "    phi1, phi2 = np.radians(lat1), np.radians(lat2)\n",
    "    dphi = np.radians(lat2 - lat1)\n",
    "    dlambda = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dphi/2)**2 + np.cos(phi1)*np.cos(phi2) * np.sin(dlambda/2)**2\n",
    "    return 2 * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "def engineer_location_features(df):\n",
    "    \n",
    "    CENTER_LAT, CENTER_LON = 41.2501, 3.4213 # coordinates of Puerta del Sol, Madrid\n",
    "    \n",
    "    df['dist_to_center_km'] = haversine_distance(\n",
    "        df['latitude'], df['longitude'], CENTER_LAT, CENTER_LON\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# 6. ADVANCED PLOTTING (DECISION MAKING)\n",
    "def generate_plots(df):\n",
    "    print(\"Generating Plots...\")\n",
    "    \n",
    "    # Plot 1: Mapbox - Price vs Location vs Rating\n",
    "    # Helps decide: \"Which neighborhoods command high prices despite lower ratings?\"\n",
    "    fig_map = px.scatter_mapbox(\n",
    "        df, \n",
    "        lat=\"latitude\", lon=\"longitude\", \n",
    "        color=\"price\", size=\"accommodates\",\n",
    "        color_continuous_scale=px.colors.sequential.Plasma, \n",
    "        range_color=[0, df['price'].quantile(0.95)],\n",
    "        size_max=15, zoom=11,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title=\"<b>Geolocation Pricing Strategy:</b> Price vs Popularity (Size)\"\n",
    "    )\n",
    "    fig_map.show()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Plot 2: Correlation Heatmap\n",
    "    # Helps decide: \"Which features actually drive Price?\"\n",
    "    # Select a subset of interesting columns\n",
    "    cols = ['price', 'dist_to_center_km', 'amenity_count', 'occupancy_rate_365']\n",
    "    corr = df[cols].corr()\n",
    "    \n",
    "    fig_corr = px.imshow(\n",
    "        corr, \n",
    "        text_auto=True, \n",
    "        aspect=\"auto\",\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        title=\"<b>Feature Importance:</b> Correlation Matrix\"\n",
    "    )\n",
    "    fig_corr.show()\n",
    "    \n",
    "    # Plot 3: Interactive Distribution (Violin)\n",
    "    # Helps decide: \"Is the price distribution in this neighborhood normal or skewed?\"\n",
    "\n",
    "    price_limit = df['price'].quantile(0.95)\n",
    "    \n",
    "    # 2. Create a temporary \"clean\" dataframe just for this plot\n",
    "    df_no_outliers = df[df['price'] < price_limit]\n",
    "    fig_vio = px.violin(\n",
    "        df_no_outliers, \n",
    "        y=\"price\", \n",
    "        x=\"neighbourhood_group_cleansed\", \n",
    "        color=\"neighbourhood_group_cleansed\",\n",
    "        box=True, \n",
    "        hover_data=df.columns,\n",
    "        title=\"<b>Market Segmentation:</b> Price Distribution by Neighbourhood\"\n",
    "    )\n",
    "    fig_vio.update_layout(showlegend=False)\n",
    "    fig_vio.show()\n",
    "\n",
    "    fig_vio2 = px.violin(\n",
    "        df_no_outliers, \n",
    "        y=\"accommodates\", \n",
    "        x=\"neighbourhood_group_cleansed\", \n",
    "        color=\"neighbourhood_group_cleansed\",\n",
    "        box=True, \n",
    "        hover_data=df.columns,\n",
    "        title=\"<b>Market Segmentation:</b> Accommodates Distribution by Neighbourhood\"\n",
    "    )\n",
    "    fig_vio2.update_layout(showlegend=False)\n",
    "    fig_vio2.show()\n",
    "\n",
    "    df['price_per_person'] = df['price'] / df['accommodates'].replace(0, 1)\n",
    "    df_no_outliers['price_per_person'] = df['price'] / df['accommodates'].replace(0, 1)\n",
    "\n",
    "    fig_vio3 = px.violin(\n",
    "        df_no_outliers, \n",
    "        y=\"price_per_person\", \n",
    "        x=\"neighbourhood_group_cleansed\", \n",
    "        color=\"neighbourhood_group_cleansed\",\n",
    "        box=True, \n",
    "        hover_data=df.columns,\n",
    "        title=\"<b>Market Segmentation:</b> Price per Person Distribution by Neighbourhood\"\n",
    "    )\n",
    "    fig_vio3.update_layout(showlegend=False)\n",
    "    fig_vio3.show()\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # PLOT 4: \"True Cost\" Map (Price per Capita)\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    # 1. Feature Engineering: Create the Price-Per-Person metric\n",
    "    # We use .replace(0, 1) to prevent crashing if 'accommodates' is ever 0\n",
    "    \n",
    "    # 2. Robust Scaling: Calculate the 95th percentile for this NEW metric\n",
    "    # (e.g., usually around $50-$100 per person)\n",
    "    ppp_limit = df['price_per_person'].quantile(0.995)\n",
    "    \n",
    "    fig_ppp = px.scatter_mapbox(\n",
    "        df, \n",
    "        lat=\"latitude\", \n",
    "        lon=\"longitude\", \n",
    "        \n",
    "        # COLOR = How expensive it feels per person (Yellow/Red is expensive)\n",
    "        color=\"price_per_person\", \n",
    "\n",
    "        # COLORS: 'Plasma' is excellent for \"Heat/Intensity\" visualization\n",
    "        color_continuous_scale=px.colors.sequential.Plasma, \n",
    "        \n",
    "        # CLAMP: Fix the scale so outliers don't wash it out\n",
    "        range_color=[0, ppp_limit],\n",
    "        \n",
    "        size_max=15, \n",
    "        zoom=11,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title=f\"<b>True Value Map:</b> Price per Person (Scale capped at ${ppp_limit:.0f})\",\n",
    "        \n",
    "        # HOVER: Show the math so you can verify\n",
    "        hover_data={'name': True, 'price': True, 'accommodates': True, 'price_per_person': ':.2f'}\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # PLOT 5: The \"Executive View\" (Aggregated by Neighborhood)\n",
    "    # ---------------------------------------------------------\n",
    "    \n",
    "    # 1. Group by Neighborhood to remove the noise of individual listings\n",
    "    neighborhood_stats = df.groupby('neighbourhood_cleansed').agg({\n",
    "        'price_per_person': 'median',       \n",
    "        'listing_id': 'count',              \n",
    "        'latitude': 'mean',                 \n",
    "        'longitude': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # 2. Rename columns for the plot\n",
    "    neighborhood_stats.rename(columns={'listing_id': 'Total Listings'}, inplace=True)\n",
    "    \n",
    "    # 3. Plot bubbles\n",
    "    fig_agg = px.scatter_mapbox(\n",
    "        neighborhood_stats, \n",
    "        lat=\"latitude\", \n",
    "        lon=\"longitude\", \n",
    "        \n",
    "        # COLOR: How expensive is the AREA?\n",
    "        color=\"price_per_person\", \n",
    "        \n",
    "        # SIZE: How many competitors are there?\n",
    "        size=\"Total Listings\",\n",
    "        \n",
    "        # TEXT: Show the name on the map\n",
    "        text=\"neighbourhood_cleansed\",\n",
    "        \n",
    "        color_continuous_scale=px.colors.sequential.Plasma,\n",
    "        size_max=40, # Make bubbles big enough to read\n",
    "        zoom=11,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title=\"<b>Strategic Overview:</b> Neighborhood Value vs. Saturation\"\n",
    "    )\n",
    "    \n",
    "    # Move the text labels so they don't block the bubbles\n",
    "    fig_agg.update_traces(textposition='top center')\n",
    "    \n",
    "    fig_agg.show()\n",
    "    \n",
    "    fig_ppp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run the pipeline\n",
    "full_df, ml_matrix = create_master_matrix(listings, calendar, reviews)\n",
    "\n",
    "# 3. Visualize\n",
    "generate_plots(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7a32c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.head()\n",
    "ml_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_target_correlation(df):\n",
    "    print(\"Generating Target Correlation Plot...\")\n",
    "    \n",
    "    # 1. Select only numeric columns\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    \n",
    "    # 2. Drop ID columns (they are random numbers, not features)\n",
    "    cols_to_drop = ['listing_id', 'id', 'scrape_id', 'host_id']\n",
    "    numeric_df = numeric_df.drop(columns=[c for c in cols_to_drop if c in numeric_df.columns])\n",
    "    \n",
    "    # 3. Calculate Correlation with Price\n",
    "    corr = numeric_df.corr()[['price']].sort_values(by='price', ascending=False)\n",
    "    \n",
    "    # 4. Remove 'price' itself (correlation is always 1.0)\n",
    "    corr = corr.drop('price')\n",
    "    \n",
    "    # 5. Plot\n",
    "    fig = px.bar(\n",
    "        corr,\n",
    "        x='price',\n",
    "        y=corr.index,\n",
    "        orientation='h',\n",
    "        color='price',\n",
    "        color_continuous_scale='RdBu_r', # Red = Positive, Blue = Negative\n",
    "        title=\"<b>Feature Importance:</b> What drives the Price up or down?\",\n",
    "        labels={'price': 'Correlation Coefficient', 'index': 'Feature'}\n",
    "    )\n",
    "    \n",
    "    # Clean up the layout\n",
    "    fig.update_layout(height=800) # Make it tall enough to read\n",
    "    fig.show()\n",
    "\n",
    "# Run it\n",
    "plot_target_correlation(ml_matrix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c31f1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_features_matrix(df):\n",
    "    print(\"Generating Scatter Matrix...\")\n",
    "    \n",
    "    # 1. Identify top 4 features correlated with Price (absolute value)\n",
    "    # We filter purely numeric columns first\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    \n",
    "    # Get top 4 correlations (excluding price itself)\n",
    "    top_cols = numeric_df.corr()['price'].abs().sort_values(ascending=False).index[1:5]\n",
    "    \n",
    "    # Add Price back to the list so we can compare\n",
    "    selected_cols = ['price'] + list(top_cols)\n",
    "    \n",
    "    # 2. Plot Scatter Matrix\n",
    "    fig = px.scatter_matrix(\n",
    "        df,\n",
    "        dimensions=selected_cols,\n",
    "        color=\"room_type\", # Color helps see if clusters are due to room type\n",
    "        title=\"<b>Linearity Test:</b> Top 4 Features vs Price\",\n",
    "        opacity=0.5, # Make dots see-through to handle overlap\n",
    "        height=900,\n",
    "        width=900\n",
    "    )\n",
    "    fig.update_traces(diagonal_visible=False) # Remove diagonal histograms for clarity\n",
    "    fig.show()\n",
    "\n",
    "# Run it\n",
    "plot_top_features_matrix(full_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a916c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_feature_correlations(df):\n",
    "    print(\"Generating Feature Correlation Matrix...\")\n",
    "\n",
    "    # 1. Select only numeric columns (your \"Xs\")\n",
    "    # We drop 'price' because we only care about X vs X here\n",
    "    cols_to_drop = ['listing_id', 'id', 'scrape_id', 'host_id', 'price', 'price_x', 'price_y']\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    \n",
    "    # Drop irrelevant columns if they exist\n",
    "    numeric_df = numeric_df.drop(columns=[c for c in cols_to_drop if c in numeric_df.columns])\n",
    "\n",
    "    # 2. Calculate the Correlation Matrix\n",
    "    corr_matrix = numeric_df.corr()\n",
    "\n",
    "    # 3. Create the Heatmap\n",
    "    fig = px.imshow(\n",
    "        corr_matrix,\n",
    "        text_auto=\".2f\", # Show the numbers (2 decimal places)\n",
    "        aspect=\"auto\",\n",
    "        color_continuous_scale='RdBu_r', # Red = Positive, Blue = Negative\n",
    "        zmin=-1, zmax=1, # Lock scale between -1 and 1\n",
    "        title=\"<b>Multicollinearity Check:</b> Correlation between Features (Xs)\",\n",
    "        labels=dict(color=\"Correlation\")\n",
    "    )\n",
    "\n",
    "    # 4. Refine Layout\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=1000, # Large square to read labels\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Features\"\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Run it\n",
    "plot_feature_correlations(ml_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4525a610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "def plot_clean_correlation_matrix(df):\n",
    "    print(\"Generating Clean Correlation Matrix (Lower Triangle Only)...\")\n",
    "\n",
    "    # 1. Select numeric columns (Xs)\n",
    "    cols_to_drop = ['listing_id', 'id', 'scrape_id', 'host_id', 'price', 'price_x', 'price_y']\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    numeric_df = numeric_df.drop(columns=[c for c in cols_to_drop if c in numeric_df.columns])\n",
    "\n",
    "    # 2. Calculate Correlation\n",
    "    corr_matrix = numeric_df.corr()\n",
    "\n",
    "    # 3. MASKING: Remove Diagonal and Upper Triangle\n",
    "    # np.triu returns the \"Upper Triangle\" of the matrix\n",
    "    # We use this mask to set those values to NaN (which Plotly will hide)\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    corr_masked = corr_matrix.mask(mask)\n",
    "\n",
    "    # 4. Plot\n",
    "    fig = px.imshow(\n",
    "        corr_masked,\n",
    "        text_auto=\".2f\",\n",
    "        aspect=\"auto\",\n",
    "        color_continuous_scale='RdBu_r', \n",
    "        zmin=-1, zmax=1,\n",
    "        title=\"<b>Multicollinearity Check:</b> Clean Correlation Matrix (Xs)\",\n",
    "        labels=dict(color=\"Correlation\")\n",
    "    )\n",
    "\n",
    "    # 5. Clean up layout to hide empty space\n",
    "    fig.update_layout(\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        xaxis_title=\"Features\",\n",
    "        yaxis_title=\"Features\",\n",
    "        plot_bgcolor='rgba(0,0,0,0)' # Transparent background for the empty part\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Run it\n",
    "plot_clean_correlation_matrix(ml_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
